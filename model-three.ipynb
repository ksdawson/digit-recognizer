{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-15T07:21:25.058365Z",
     "iopub.status.busy": "2026-02-15T07:21:25.058141Z",
     "iopub.status.idle": "2026-02-15T07:21:26.196920Z",
     "shell.execute_reply": "2026-02-15T07:21:26.196326Z",
     "shell.execute_reply.started": "2026-02-15T07:21:25.058343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T07:21:26.198757Z",
     "iopub.status.busy": "2026-02-15T07:21:26.198443Z",
     "iopub.status.idle": "2026-02-15T07:21:33.641722Z",
     "shell.execute_reply": "2026-02-15T07:21:33.640714Z",
     "shell.execute_reply.started": "2026-02-15T07:21:26.198733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DigitRecognizerDataset(Dataset):\n",
    "    def __init__(self, dataframe, is_test=False):\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        if not self.is_test:\n",
    "            # Train set: First column is label, rest are pixels\n",
    "            self.labels = torch.tensor(dataframe.iloc[:, 0].values, dtype=torch.long)\n",
    "            data = dataframe.iloc[:, 1:].values\n",
    "        else:\n",
    "            # Test set: All columns are pixels, no labels provided\n",
    "            self.labels = None \n",
    "            data = dataframe.values\n",
    "            \n",
    "        # Reshape the data to (Batch, Channel, Height, Width)\n",
    "        self.features = torch.tensor(data, dtype=torch.float32).view(-1, 1, 28, 28)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            return self.features[idx]\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T07:21:33.643130Z",
     "iopub.status.busy": "2026-02-15T07:21:33.642659Z",
     "iopub.status.idle": "2026-02-15T07:21:38.660669Z",
     "shell.execute_reply": "2026-02-15T07:21:38.660013Z",
     "shell.execute_reply.started": "2026-02-15T07:21:33.643095Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_set_filepath = os.path.join(os.getcwd(), 'train.csv')\n",
    "test_set_filepath = os.path.join(os.getcwd(), 'test.csv')\n",
    "\n",
    "# Load datasets\n",
    "base_train_df = pd.read_csv(train_set_filepath)\n",
    "if os.path.exists(test_set_filepath):\n",
    "    base_test_df = pd.read_csv(test_set_filepath)\n",
    "else:\n",
    "    base_test_df = None\n",
    "\n",
    "# Only the train set has labels so we need to split that into our real train and test sets\n",
    "train_df, val_df = train_test_split(base_train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to torch datasets\n",
    "train_set = DigitRecognizerDataset(train_df, is_test=False)\n",
    "val_set = DigitRecognizerDataset(val_df, is_test=False)\n",
    "if base_test_df:\n",
    "    test_set = DigitRecognizerDataset(base_test_df, is_test=True)\n",
    "else:\n",
    "    test_set = None\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "if test_set:\n",
    "    test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "else:\n",
    "    test_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T07:21:38.662077Z",
     "iopub.status.busy": "2026-02-15T07:21:38.661816Z",
     "iopub.status.idle": "2026-02-15T07:21:38.671027Z",
     "shell.execute_reply": "2026-02-15T07:21:38.670265Z",
     "shell.execute_reply.started": "2026-02-15T07:21:38.662053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Block 1: Input 1x28x28 -> Output 32x14x14\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.drop1 = nn.Dropout2d(0.25)\n",
    "\n",
    "        # Block 2: Input 32x14x14 -> Output 64x7x7\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.drop2 = nn.Dropout2d(0.25)\n",
    "\n",
    "        # Fully Connected Classifier\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        # Block 2\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        # Flatten and Classify\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T07:21:38.672542Z",
     "iopub.status.busy": "2026-02-15T07:21:38.672036Z",
     "iopub.status.idle": "2026-02-15T07:21:38.805705Z",
     "shell.execute_reply": "2026-02-15T07:21:38.805024Z",
     "shell.execute_reply.started": "2026-02-15T07:21:38.672505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "\n",
    "cnn = CNN().to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T07:21:39.399606Z",
     "iopub.status.busy": "2026-02-15T07:21:39.399372Z",
     "iopub.status.idle": "2026-02-15T07:21:39.404709Z",
     "shell.execute_reply": "2026-02-15T07:21:39.404114Z",
     "shell.execute_reply.started": "2026-02-15T07:21:39.399584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Validation\n",
    "def validate_model(model):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels) # Calculate loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}% | Val Loss: {avg_loss:.4f}')\n",
    "    return avg_loss # Return this for the scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-02-15T08:22:42.685Z",
     "iopub.execute_input": "2026-02-15T07:21:39.405858Z",
     "iopub.status.busy": "2026-02-15T07:21:39.405559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.699\n",
      "[1,   200] loss: 0.249\n",
      "[1,   300] loss: 0.185\n",
      "[1,   400] loss: 0.153\n",
      "[1,   500] loss: 0.135\n",
      "[1,   600] loss: 0.141\n",
      "[1,   700] loss: 0.114\n",
      "[1,   800] loss: 0.106\n",
      "[1,   900] loss: 0.107\n",
      "[1,  1000] loss: 0.097\n",
      "Accuracy: 98.58% | Val Loss: 0.0452\n",
      "End of Epoch 1 - Learning Rate: 0.001\n",
      "End of Epoch 1\n",
      "[2,   100] loss: 0.092\n",
      "[2,   200] loss: 0.079\n",
      "[2,   300] loss: 0.099\n",
      "[2,   400] loss: 0.089\n",
      "[2,   500] loss: 0.086\n",
      "[2,   600] loss: 0.070\n",
      "[2,   700] loss: 0.087\n",
      "[2,   800] loss: 0.077\n",
      "[2,   900] loss: 0.077\n",
      "[2,  1000] loss: 0.073\n",
      "Accuracy: 99.08% | Val Loss: 0.0308\n",
      "End of Epoch 2 - Learning Rate: 0.001\n",
      "End of Epoch 2\n",
      "[3,   100] loss: 0.071\n",
      "[3,   200] loss: 0.064\n",
      "[3,   300] loss: 0.071\n",
      "[3,   400] loss: 0.066\n",
      "[3,   500] loss: 0.051\n",
      "[3,   600] loss: 0.061\n",
      "[3,   700] loss: 0.065\n",
      "[3,   800] loss: 0.054\n",
      "[3,   900] loss: 0.065\n",
      "[3,  1000] loss: 0.056\n",
      "Accuracy: 99.24% | Val Loss: 0.0230\n",
      "End of Epoch 3 - Learning Rate: 0.001\n",
      "End of Epoch 3\n",
      "[4,   100] loss: 0.052\n",
      "[4,   200] loss: 0.051\n",
      "[4,   300] loss: 0.059\n",
      "[4,   400] loss: 0.058\n",
      "[4,   500] loss: 0.051\n",
      "[4,   600] loss: 0.047\n",
      "[4,   700] loss: 0.067\n",
      "[4,   800] loss: 0.052\n",
      "[4,   900] loss: 0.058\n",
      "[4,  1000] loss: 0.059\n",
      "Accuracy: 99.29% | Val Loss: 0.0230\n",
      "End of Epoch 4 - Learning Rate: 0.001\n",
      "End of Epoch 4\n",
      "[5,   100] loss: 0.047\n",
      "[5,   200] loss: 0.046\n",
      "[5,   300] loss: 0.048\n",
      "[5,   400] loss: 0.042\n",
      "[5,   500] loss: 0.044\n",
      "[5,   600] loss: 0.045\n",
      "[5,   700] loss: 0.046\n",
      "[5,   800] loss: 0.050\n",
      "[5,   900] loss: 0.055\n",
      "[5,  1000] loss: 0.040\n",
      "Accuracy: 99.17% | Val Loss: 0.0223\n",
      "End of Epoch 5 - Learning Rate: 0.001\n",
      "End of Epoch 5\n",
      "[6,   100] loss: 0.033\n",
      "[6,   200] loss: 0.056\n",
      "[6,   300] loss: 0.038\n",
      "[6,   400] loss: 0.037\n",
      "[6,   500] loss: 0.039\n",
      "[6,   600] loss: 0.033\n",
      "[6,   700] loss: 0.048\n",
      "[6,   800] loss: 0.051\n",
      "[6,   900] loss: 0.044\n",
      "[6,  1000] loss: 0.045\n",
      "Accuracy: 99.12% | Val Loss: 0.0260\n",
      "End of Epoch 6 - Learning Rate: 0.001\n",
      "End of Epoch 6\n",
      "[7,   100] loss: 0.033\n",
      "[7,   200] loss: 0.037\n",
      "[7,   300] loss: 0.042\n",
      "[7,   400] loss: 0.041\n",
      "[7,   500] loss: 0.043\n",
      "[7,   600] loss: 0.044\n",
      "[7,   700] loss: 0.031\n",
      "[7,   800] loss: 0.045\n",
      "[7,   900] loss: 0.032\n",
      "[7,  1000] loss: 0.040\n",
      "Accuracy: 99.35% | Val Loss: 0.0195\n",
      "End of Epoch 7 - Learning Rate: 0.001\n",
      "End of Epoch 7\n",
      "[8,   100] loss: 0.028\n",
      "[8,   200] loss: 0.037\n",
      "[8,   300] loss: 0.039\n",
      "[8,   400] loss: 0.043\n",
      "[8,   500] loss: 0.033\n",
      "[8,   600] loss: 0.032\n",
      "[8,   700] loss: 0.025\n",
      "[8,   800] loss: 0.043\n",
      "[8,   900] loss: 0.029\n",
      "[8,  1000] loss: 0.039\n",
      "Accuracy: 99.46% | Val Loss: 0.0184\n",
      "End of Epoch 8 - Learning Rate: 0.001\n",
      "End of Epoch 8\n",
      "[9,   100] loss: 0.029\n",
      "[9,   200] loss: 0.036\n",
      "[9,   300] loss: 0.031\n",
      "[9,   400] loss: 0.021\n",
      "[9,   500] loss: 0.037\n",
      "[9,   600] loss: 0.037\n",
      "[9,   700] loss: 0.040\n",
      "[9,   800] loss: 0.037\n",
      "[9,   900] loss: 0.042\n",
      "[9,  1000] loss: 0.046\n",
      "Accuracy: 99.21% | Val Loss: 0.0233\n",
      "End of Epoch 9 - Learning Rate: 0.001\n",
      "End of Epoch 9\n",
      "[10,   100] loss: 0.028\n",
      "[10,   200] loss: 0.023\n",
      "[10,   300] loss: 0.028\n",
      "[10,   400] loss: 0.026\n",
      "[10,   500] loss: 0.034\n",
      "[10,   600] loss: 0.038\n",
      "[10,   700] loss: 0.029\n",
      "[10,   800] loss: 0.036\n",
      "[10,   900] loss: 0.030\n",
      "[10,  1000] loss: 0.022\n",
      "Accuracy: 99.31% | Val Loss: 0.0196\n",
      "End of Epoch 10 - Learning Rate: 0.001\n",
      "End of Epoch 10\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    cnn.train() # Set model back to training mode\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Run validation and get the loss\n",
    "    current_val_loss = validate_model(cnn)\n",
    "\n",
    "    # Step the scheduler based on the validation loss\n",
    "    scheduler.step(current_val_loss)\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'End of Epoch {epoch + 1} - Learning Rate: {current_lr}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "execution_failed": "2026-02-15T08:22:42.685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Submission\n",
    "submission_df = None\n",
    "if test_loader:\n",
    "    submission_data = {'ImageId': [], 'Label': []}\n",
    "\n",
    "    cnn.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        idx = 1\n",
    "        for images in test_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            logits = cnn(images)\n",
    "            \n",
    "            # Get the index of the max accumulated log-probability\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "\n",
    "            preds = predicted.cpu().numpy()\n",
    "\n",
    "            for p in preds:\n",
    "                submission_data['ImageId'].append(idx)\n",
    "                submission_data['Label'].append(p)\n",
    "                idx += 1\n",
    "\n",
    "    # Save to csv\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "execution_failed": "2026-02-15T08:22:42.685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if submission_df:\n",
    "    # 1. Check for missing values\n",
    "    missing_values = submission_df.isnull().sum().sum()\n",
    "\n",
    "    # 2. Check for correct label range (0-9)\n",
    "    invalid_labels = submission_df[(submission_df['Label'] < 0) | (submission_df['Label'] > 9)]\n",
    "\n",
    "    # 3. Final Verification Report\n",
    "    print(\"--- Submission Verification ---\")\n",
    "    print(f\"Total Rows: {len(submission_df)}\")\n",
    "    print(f\"Missing Values: {missing_values}\")\n",
    "    print(f\"Invalid Labels Found: {len(invalid_labels)}\")\n",
    "    print(f\"Column Names: {list(submission_df.columns)}\")\n",
    "\n",
    "    if len(submission_df) == len(real_test_df) and missing_values == 0 and len(invalid_labels) == 0:\n",
    "        print(\"\\n✅ Verification Passed! Your file is ready for submission.\")\n",
    "    else:\n",
    "        print(\"\\n❌ Verification Failed. Please check the counts above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "execution_failed": "2026-02-15T08:22:42.685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if submission_df:    \n",
    "    # Get one batch from the test loader\n",
    "    images = next(iter(test_loader))\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Plot the first image in the batch\n",
    "    plt.imshow(images[0].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Predicted Label: {predicted[0].item()}')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
